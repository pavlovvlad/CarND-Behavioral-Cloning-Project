{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Behavioral Cloning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: include libraries, define helper functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Lambda, Cropping2D\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "### helper functions\n",
    "# function implements the data augmentation\n",
    "def data_augmentation(images, angles):\n",
    "    aug_images, aug_angles = [], []\n",
    "        \n",
    "    # flipping\n",
    "    for image, angle in zip(images, angles):\n",
    "        aug_images.append(image)\n",
    "        aug_angles.append(angle)\n",
    "        aug_images.append(cv2.flip(image,1))\n",
    "        aug_angles.append(angle*-1.0)\n",
    "\n",
    "    return aug_images, aug_angles\n",
    "\n",
    "# function with usnig of the generator in sense of python \n",
    "def generator(samples, batch_size=128):\n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        sklearn.utils.shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                name = batch_sample[0].split('/')[-1]\n",
    "                center_image = cv2.imread(name)\n",
    "                center_angle = float(batch_sample[3])\n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "            # save the input data as numpy-arrays\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            print(\"Offset:\", offset)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "            #yield images, angles\n",
    "\n",
    "### constants \n",
    "# define sets for trainig and for validation\n",
    "TRAIN_RATIO = 0.2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "# rows pixels from the top of the image\n",
    "TOP_RAWS = 74\n",
    "# rows pixels from the bottom of the image\n",
    "BOTTOM_RAWS = 20\n",
    "# columns of pixels from the left of the image\n",
    "LEFT_COLS = 0\n",
    "# columns of pixels from the right of the image\n",
    "RIGHT_COLS = 0\n",
    "\n",
    "# input image format\n",
    "ch, row, col = 3, 160, 320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Load images and prepeare sets for training and validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for training:  3348\n",
      "Number of samples for validation:  838\n"
     ]
    }
   ],
   "source": [
    "### main run routine\n",
    "samples = []\n",
    "with open('./data_own/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "\n",
    "# split sets for training and validation\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=TRAIN_RATIO)\n",
    "print(\"Number of samples for training: \", len(train_samples))\n",
    "print(\"Number of samples for validation: \", len(validation_samples))\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=BATCH_SIZE)\n",
    "validation_generator = generator(validation_samples, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data Set (normalization, grayscale, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Pre-process all data\n",
    "# grayscale conversion\n",
    "# * img: input images to preprocess\n",
    "# * return: preprocessed images\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "# adaptive histogram equalization\n",
    "# * img: input images to preprocess\n",
    "# * clahe: CLAHE object (Contrast Limited Adaptive Histogram Equalization)\n",
    "# * return: preprocessed images\n",
    "def hist_equalization(img, clahe):\n",
    "    img = img.astype(np.uint8)\n",
    "    img = img.squeeze()\n",
    "    #img = cv2.equalizeHist(img)\n",
    "    img = clahe.apply(img)\n",
    "    return img\n",
    "\n",
    "# preprocess function\n",
    "# * img_arr: input array of images to process\n",
    "# * return: output array with preprocessed images\n",
    "def preprocess_image_array(img_arr):\n",
    "    # init the new numpy array\n",
    "    img_arr_new = np.empty(img_arr.shape[:3])\n",
    "    # create a CLAHE object\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    for i in range (len(img_arr)):\n",
    "        # convert the input RGB-images in grayscale-format\n",
    "        img_arr_new[i] = rgb2gray(img_arr[i]) # cv2.cvtColor(img_arr[i], cv2.COLOR_BGR2GRAY)\n",
    "        # apply the histogram equalization\n",
    "        img_arr_new[i] = hist_equalization(img_arr_new[i], clahe)\n",
    "    # normalize     \n",
    "    img_arr_new = img_arr_new / 255 #(img_arr_new - 128) / 128\n",
    "    return img_arr_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# define the architecture of the model based on NVIDIA CNN-Architecture\n",
    "# pre-process incoming data, trimming of the image \n",
    "model.add(Cropping2D(cropping=((TOP_RAWS,BOTTOM_RAWS),(LEFT_COLS,RIGHT_COLS)), input_shape=(row, col, ch)))\n",
    "# pre-process incoming data, centered around zero with small standard deviation \n",
    "model.add(Lambda(lambda x: ( x / 255.0 ) - 0.5))\n",
    "\n",
    "# Layer Conv1, input shape: 3x66x320\n",
    "model.add(Convolution2D(24, (5, 5), strides=2))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer Conv2, input shape: 24x31x158 \n",
    "model.add(Convolution2D(36, (5, 5), strides=2))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer Conv3, input shape: 36x14x77\n",
    "model.add(Convolution2D(48, (5, 5), strides=2))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer Conv4, input shape: 48x5x36\n",
    "model.add(Convolution2D(64, (3, 3)))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer Conv5, input shape: 64x3x34\n",
    "model.add(Convolution2D(64, (3, 3)))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer Conv6, input shape: 64x1x32\n",
    "model.add(Flatten())\n",
    "# Layer Fully connected 7, input shape: 2048\n",
    "model.add(Dense(160))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "# Layer Fully connected 8, input shape: 160\n",
    "model.add(Dense(80))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "# Layer Fully connected 9, input shape: 80\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "# Layer Fully connected 10, input shape: 16\n",
    "model.add(Dense(1))\n",
    "#print(model.get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation\n",
    "sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: The semantics of the Keras 2 argument  `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Update your method calls accordingly.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., epochs=15, validation_steps=589, verbose=1, validation_data=<generator..., steps_per_epoch=1372)`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Offset: 0\n",
      "Offset: 128\n",
      "   1/1372 [..............................] - ETA: 22429s - loss: 0.0269Offset: 256\n",
      "   2/1372 [..............................] - ETA: 15018s - loss: 0.0417Offset: 384\n",
      "   3/1372 [..............................] - ETA: 12509s - loss: 0.0390Offset: 512\n",
      "   4/1372 [..............................] - ETA: 11281s - loss: 0.0360Offset: 640\n",
      "   6/1372 [..............................] - ETA: 10067s - loss: 0.0327\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bOffset: 768\n",
      "   7/1372 [..............................] - ETA: 9715s - loss: 0.0317 Offset: 896\n",
      "   8/1372 [..............................] - ETA: 9852s - loss: 0.0314Offset: 1024\n",
      "   9/1372 [..............................] - ETA: 10279s - loss: 0.0304Offset: 1152\n",
      "Offset: 1280\n",
      "Offset: 0\n",
      "  10/1372 [..............................] - ETA: 10442s - loss: 0.0296Offset: 128\n",
      "Offset: 256\n",
      "Offset: 384\n",
      "Offset: 512\n",
      "Offset: 640\n",
      "Offset: 768\n",
      "  11/1372 [..............................] - ETA: 10174s - loss: 0.0293Offset: 896\n",
      "Offset: 1024\n",
      "Offset: 1152\n",
      "Offset: 1280\n",
      "  12/1372 [..............................] - ETA: 10164s - loss: 0.0292Offset: 0\n",
      "  13/1372 [..............................] - ETA: 10072s - loss: 0.0297Offset: 128\n",
      "  14/1372 [..............................] - ETA: 10046s - loss: 0.0302Offset: 256\n",
      "  15/1372 [..............................] - ETA: 10011s - loss: 0.0300Offset: 384\n",
      "  16/1372 [..............................] - ETA: 10188s - loss: 0.0296Offset: 512\n",
      "  17/1372 [..............................] - ETA: 10290s - loss: 0.0295Offset: 640\n",
      "  18/1372 [..............................] - ETA: 10251s - loss: 0.0293Offset: 768\n",
      "  19/1372 [..............................] - ETA: 10183s - loss: 0.0292Offset: 896\n",
      "  20/1372 [..............................] - ETA: 10216s - loss: 0.0289Offset: 1024\n",
      "  21/1372 [..............................] - ETA: 10151s - loss: 0.0285Offset: 1152\n",
      "  22/1372 [..............................] - ETA: 9994s - loss: 0.0285 Offset: 1280\n",
      "  23/1372 [..............................] - ETA: 9965s - loss: 0.0285Offset: 0\n",
      "  24/1372 [..............................] - ETA: 9925s - loss: 0.0288Offset: 128\n",
      "  25/1372 [..............................] - ETA: 9919s - loss: 0.0290Offset: 256\n",
      "  26/1372 [..............................] - ETA: 9912s - loss: 0.0290Offset: 384\n",
      "  27/1372 [..............................] - ETA: 9866s - loss: 0.0288Offset: 512\n",
      "  28/1372 [..............................] - ETA: 9867s - loss: 0.0287Offset: 640\n",
      "  29/1372 [..............................] - ETA: 9854s - loss: 0.0286Offset: 768\n",
      "  30/1372 [..............................] - ETA: 9875s - loss: 0.0286Offset: 896\n",
      "  31/1372 [..............................] - ETA: 9879s - loss: 0.0284Offset: 1024\n",
      "  32/1372 [..............................] - ETA: 10645s - loss: 0.0282Offset: 1152\n",
      "  33/1372 [..............................] - ETA: 10898s - loss: 0.0282Offset: 1280\n",
      "  34/1372 [..............................] - ETA: 11540s - loss: 0.0281Offset: 0\n",
      "  35/1372 [..............................] - ETA: 11952s - loss: 0.0284Offset: 128\n",
      "  36/1372 [..............................] - ETA: 11886s - loss: 0.0285Offset: 256\n",
      "  37/1372 [..............................] - ETA: 11807s - loss: 0.0285Offset: 384\n",
      "  38/1372 [..............................] - ETA: 11784s - loss: 0.0284Offset: 512\n",
      "  39/1372 [..............................] - ETA: 11685s - loss: 0.0283Offset: 640\n",
      "  40/1372 [..............................] - ETA: 11571s - loss: 0.0282Offset: 768\n",
      "  41/1372 [..............................] - ETA: 11479s - loss: 0.0283Offset: 896\n",
      "  42/1372 [..............................] - ETA: 11410s - loss: 0.0281Offset: 1024\n",
      "  43/1372 [..............................] - ETA: 11330s - loss: 0.0280Offset: 1152\n",
      "  44/1372 [..............................] - ETA: 11215s - loss: 0.0280Offset: 1280\n",
      "  45/1372 [..............................] - ETA: 11142s - loss: 0.0280Offset: 0\n",
      "  46/1372 [>.............................] - ETA: 11098s - loss: 0.0281Offset: 128\n",
      "  47/1372 [>.............................] - ETA: 11135s - loss: 0.0283Offset: 256\n",
      "  48/1372 [>.............................] - ETA: 11078s - loss: 0.0282Offset: 384\n",
      "  49/1372 [>.............................] - ETA: 11040s - loss: 0.0281Offset: 512\n",
      "  50/1372 [>.............................] - ETA: 11013s - loss: 0.0281Offset: 640\n",
      "  51/1372 [>.............................] - ETA: 11013s - loss: 0.0280Offset: 768\n",
      "  52/1372 [>.............................] - ETA: 10999s - loss: 0.0280Offset: 896\n",
      "  53/1372 [>.............................] - ETA: 10966s - loss: 0.0279Offset: 1024\n",
      "  54/1372 [>.............................] - ETA: 10937s - loss: 0.0278Offset: 1152\n",
      "  55/1372 [>.............................] - ETA: 10874s - loss: 0.0278Offset: 1280\n",
      "  56/1372 [>.............................] - ETA: 10853s - loss: 0.0278Offset: 0\n",
      "  57/1372 [>.............................] - ETA: 10824s - loss: 0.0279Offset: 128\n",
      "  58/1372 [>.............................] - ETA: 10803s - loss: 0.0280Offset: 256\n",
      "  59/1372 [>.............................] - ETA: 10777s - loss: 0.0280Offset: 384\n",
      "  60/1372 [>.............................] - ETA: 10758s - loss: 0.0279Offset: 512\n",
      "  61/1372 [>.............................] - ETA: 10758s - loss: 0.0279Offset: 640\n",
      "  62/1372 [>.............................] - ETA: 10742s - loss: 0.0278Offset: 768\n",
      "  63/1372 [>.............................] - ETA: 10758s - loss: 0.0278Offset: 896\n",
      "  64/1372 [>.............................] - ETA: 10760s - loss: 0.0277Offset: 1024\n",
      "  65/1372 [>.............................] - ETA: 10729s - loss: 0.0276Offset: 1152\n",
      "  66/1372 [>.............................] - ETA: 10671s - loss: 0.0276Offset: 1280\n",
      "  67/1372 [>.............................] - ETA: 10663s - loss: 0.0275Offset: 0\n",
      "  68/1372 [>.............................] - ETA: 10652s - loss: 0.0276Offset: 128\n",
      "  69/1372 [>.............................] - ETA: 10740s - loss: 0.0277Offset: 256\n",
      "  70/1372 [>.............................] - ETA: 11117s - loss: 0.0277Offset: 384\n",
      "  71/1372 [>.............................] - ETA: 11594s - loss: 0.0276Offset: 512\n",
      "  72/1372 [>.............................] - ETA: 11787s - loss: 0.0276Offset: 640\n",
      "  73/1372 [>.............................] - ETA: 11762s - loss: 0.0275Offset: 768\n",
      "  74/1372 [>.............................] - ETA: 11731s - loss: 0.0275Offset: 896\n",
      "  75/1372 [>.............................] - ETA: 11704s - loss: 0.0274Offset: 1024\n",
      "  76/1372 [>.............................] - ETA: 11682s - loss: 0.0273Offset: 1152\n",
      "  77/1372 [>.............................] - ETA: 11622s - loss: 0.0273Offset: 1280\n",
      "  78/1372 [>.............................] - ETA: 11582s - loss: 0.0273Offset: 0\n",
      "  79/1372 [>.............................] - ETA: 11553s - loss: 0.0273Offset: 128\n",
      "  80/1372 [>.............................] - ETA: 11525s - loss: 0.0274Offset: 256\n",
      "  81/1372 [>.............................] - ETA: 11497s - loss: 0.0274Offset: 384\n",
      "  82/1372 [>.............................] - ETA: 11475s - loss: 0.0273Offset: 512\n",
      "  83/1372 [>.............................] - ETA: 11435s - loss: 0.0273Offset: 640\n",
      "  84/1372 [>.............................] - ETA: 11407s - loss: 0.0273Offset: 768\n",
      "  85/1372 [>.............................] - ETA: 11379s - loss: 0.0273Offset: 896\n",
      "  86/1372 [>.............................] - ETA: 11356s - loss: 0.0272Offset: 1024\n",
      "  87/1372 [>.............................] - ETA: 11341s - loss: 0.0271Offset: 1152\n",
      "  88/1372 [>.............................] - ETA: 11283s - loss: 0.0271Offset: 1280\n",
      "  89/1372 [>.............................] - ETA: 11255s - loss: 0.0271Offset: 0\n",
      "  90/1372 [>.............................] - ETA: 11233s - loss: 0.0272Offset: 128\n",
      "  91/1372 [>.............................] - ETA: 11211s - loss: 0.0272Offset: 256\n",
      "  92/1372 [=>............................] - ETA: 11194s - loss: 0.0272Offset: 384\n",
      "  93/1372 [=>............................] - ETA: 11181s - loss: 0.0272Offset: 512\n",
      "  94/1372 [=>............................] - ETA: 11156s - loss: 0.0271Offset: 640\n",
      "  95/1372 [=>............................] - ETA: 11128s - loss: 0.0271Offset: 768\n",
      "  96/1372 [=>............................] - ETA: 11093s - loss: 0.0271Offset: 896\n",
      "  97/1372 [=>............................] - ETA: 11070s - loss: 0.0270Offset: 1024\n",
      "  98/1372 [=>............................] - ETA: 11037s - loss: 0.0269Offset: 1152\n",
      "  99/1372 [=>............................] - ETA: 10985s - loss: 0.0270Offset: 1280\n",
      " 100/1372 [=>............................] - ETA: 10960s - loss: 0.0269Offset: 0\n",
      " 101/1372 [=>............................] - ETA: 11029s - loss: 0.0270Offset: 128\n",
      " 102/1372 [=>............................] - ETA: 11176s - loss: 0.0271Offset: 256\n",
      " 103/1372 [=>............................] - ETA: 11361s - loss: 0.0270Offset: 384\n",
      " 104/1372 [=>............................] - ETA: 11545s - loss: 0.0270Offset: 512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-54d612e10afa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory_object\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m             \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m             \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;31m### print the keys contained in the history object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1875\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pvlad\\Miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history_object = model.fit_generator(train_generator, samples_per_epoch= \\\n",
    "            len(train_samples), validation_data=validation_generator, \\\n",
    "            nb_val_samples=len(validation_samples), nb_epoch=EPOCHS, verbose=1)\n",
    "\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')   \n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save('model.h5')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Visualize the Neural Network's State with Test Images\n",
    "\n",
    " This Section is not required to complete but acts as an additional excersise for understaning the output of a neural network's weights. While neural networks can be a great learning device they are often referred to as a black box. We can understand what the weights of a neural network look like better by plotting their feature maps. After successfully training your neural network you can see what it's feature maps look like by plotting the output of the network's weight layers in response to a test stimuli image. From these plotted feature maps, it's possible to see what characteristics of an image the network finds interesting. For a sign, maybe the inner network feature maps react with high activation to the sign's boundary outline or to the contrast in the sign's painted symbol.\n",
    "\n",
    " Provided for you below is the function code that allows you to get the visualization output of any tensorflow weight layer you want. The inputs to the function should be a stimuli image, one used during training or a new one you provided, and then the tensorflow variable name that represents the layer's state during the training process, for instance if you wanted to see what the [LeNet lab's](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) feature maps looked like for it's second convolutional layer you could enter conv2 as the tf_activation variable.\n",
    "\n",
    "For an example of what feature map outputs look like, check out NVIDIA's results in their paper [End-to-End Deep Learning for Self-Driving Cars](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/) in the section Visualization of internal CNN State. NVIDIA was able to show that their network's inner weights had high activations to road boundary lines by comparing feature maps from an image with a clear path to one without. Try experimenting with a similar test to show that your trained network's weights are looking for interesting features, whether it's looking at differences in feature maps from images with or without a sign, or even what feature maps look like in a trained network vs a completely untrained one on the same sign image.\n",
    "\n",
    "<figure>\n",
    " <img src=\"visualize_cnn.png\" width=\"380\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output should look something like this (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Visualize your network's feature maps here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# image_input: the test image being fed into the network to produce the feature maps\n",
    "# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n",
    "# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n",
    "# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "\n",
    "import math\n",
    "\n",
    "#--------------------------\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it maybe having trouble accessing the variable from inside a function\n",
    "    #activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n",
    "    activation = sess.run(tf_activation, feed_dict={x: image_input})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(8,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    #conv2_map = sess.run(conv2_W, feed_dict={x: X_test_gray_own})\n",
    "    #print(sess.run(conv2_W))\n",
    "    outputFeatureMap([X_test_gray_own[6]], conv1, plt_num=1)\n",
    "    outputFeatureMap([X_test_gray_own[6]], conv2, plt_num=2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
