{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Behavioral Cloning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: include libraries, define helper functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Lambda, Cropping2D\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "### helper functions\n",
    "# function implements the data augmentation\n",
    "def data_augmentation(images, angles):\n",
    "    aug_images, aug_angles = [], []\n",
    "        \n",
    "    # flipping\n",
    "    for image, angle in zip(images, angles):\n",
    "        aug_images.append(image)\n",
    "        aug_angles.append(angle)\n",
    "        aug_images.append(cv2.flip(image,1))\n",
    "        aug_angles.append(angle*-1.0)\n",
    "\n",
    "    return aug_images, aug_angles\n",
    "\n",
    "# function with using of the generator in sense of python \n",
    "def generator(samples, batch_size=128):\n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        sklearn.utils.shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                name = batch_sample[0].split('\\\\')[-1]\n",
    "                path = './data_own/IMG/' + name\n",
    "                center_image = cv2.imread(path)\n",
    "                center_angle = float(batch_sample[3])\n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "            # save the input data as numpy-arrays\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            #print(path)\n",
    "            #print(\"Offset:\", offset)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "            #yield images, angles\n",
    "\n",
    "### constants \n",
    "# define sets for trainig and for validation\n",
    "TRAIN_RATIO = 0.2\n",
    "BATCH_SIZE = 128 # 32 #64 \n",
    "EPOCHS = 15\n",
    "# rows pixels from the top of the image\n",
    "TOP_RAWS = 74\n",
    "# rows pixels from the bottom of the image\n",
    "BOTTOM_RAWS = 20\n",
    "# columns of pixels from the left of the image\n",
    "LEFT_COLS = 0\n",
    "# columns of pixels from the right of the image\n",
    "RIGHT_COLS = 0\n",
    "\n",
    "# input image format\n",
    "ch, row, col = 3, 160, 320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Load images and prepeare sets for training and validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for training:  4567\n",
      "Number of samples for validation:  1142\n"
     ]
    }
   ],
   "source": [
    "### main run routine\n",
    "samples = []\n",
    "with open('./data_own/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "\n",
    "# split sets for training and validation\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=TRAIN_RATIO)\n",
    "print(\"Number of samples for training: \", len(train_samples))\n",
    "print(\"Number of samples for validation: \", len(validation_samples))\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=BATCH_SIZE)\n",
    "validation_generator = generator(validation_samples, batch_size=BATCH_SIZE)\n",
    "\n",
    "#next(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data Set (normalization, grayscale, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Pre-process all data\n",
    "# grayscale conversion\n",
    "# * img: input images to preprocess\n",
    "# * return: preprocessed images\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "# adaptive histogram equalization\n",
    "# * img: input images to preprocess\n",
    "# * clahe: CLAHE object (Contrast Limited Adaptive Histogram Equalization)\n",
    "# * return: preprocessed images\n",
    "def hist_equalization(img, clahe):\n",
    "    img = img.astype(np.uint8)\n",
    "    img = img.squeeze()\n",
    "    #img = cv2.equalizeHist(img)\n",
    "    img = clahe.apply(img)\n",
    "    return img\n",
    "\n",
    "# preprocess function\n",
    "# * img_arr: input array of images to process\n",
    "# * return: output array with preprocessed images\n",
    "def preprocess_image_array(img_arr):\n",
    "    # init the new numpy array\n",
    "    img_arr_new = np.empty(img_arr.shape[:3])\n",
    "    # create a CLAHE object\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    for i in range (len(img_arr)):\n",
    "        # convert the input RGB-images in grayscale-format\n",
    "        img_arr_new[i] = rgb2gray(img_arr[i]) # cv2.cvtColor(img_arr[i], cv2.COLOR_BGR2GRAY)\n",
    "        # apply the histogram equalization\n",
    "        img_arr_new[i] = hist_equalization(img_arr_new[i], clahe)\n",
    "    # normalize     \n",
    "    img_arr_new = img_arr_new / 255 #(img_arr_new - 128) / 128\n",
    "    return img_arr_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# define the architecture of the model based on NVIDIA CNN-Architecture\n",
    "# pre-process incoming data, trimming of the image \n",
    "model.add(Cropping2D(cropping=((TOP_RAWS,BOTTOM_RAWS),(LEFT_COLS,RIGHT_COLS)), input_shape=(row, col, ch)))\n",
    "# pre-process incoming data, centered around zero with small standard deviation \n",
    "model.add(Lambda(lambda x: ( x / 255.0 ) - 0.5))\n",
    "\n",
    "\n",
    "# Layer Conv1, input shape: 3x66x320\n",
    "model.add(Convolution2D(24, 5, 5, subsample=(2,2)))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer Conv2, input shape: 24x31x158 \n",
    "model.add(Convolution2D(36, 5, 5, subsample=(2,2)))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer Conv3, input shape: 36x14x77\n",
    "model.add(Convolution2D(48, 5, 5, subsample=(2,2)))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer Conv4, input shape: 48x5x36\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer Conv5, input shape: 64x3x34\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "    \n",
    "# Layer Conv6, input shape: 64x1x32\n",
    "model.add(Flatten())\n",
    "# Layer Fully connected 7, input shape: 2048\n",
    "model.add(Dense(160))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "# Layer Fully connected 8, input shape: 160\n",
    "model.add(Dense(80))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "# Layer Fully connected 9, input shape: 80\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "# Layer Fully connected 10, input shape: 16\n",
    "model.add(Dense(1))\n",
    "\n",
    "# visualize the model\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(model, to_file='model.png',show_shapes = True)\n",
    "print(\"Model plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation\n",
    "sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history_object = model.fit_generator(train_generator, samples_per_epoch= \\\n",
    "            len(train_samples), validation_data=validation_generator, \\\n",
    "            nb_val_samples=len(validation_samples), nb_epoch=EPOCHS, verbose=1)\n",
    "\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')   \n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save('model.h5')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualize the Neural Network's State with Test Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# image_input: the test image being fed into the network to produce the feature maps\n",
    "# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n",
    "# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n",
    "# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "\n",
    "import math\n",
    "\n",
    "#--------------------------\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it maybe having trouble accessing the variable from inside a function\n",
    "    #activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n",
    "    activation = sess.run(tf_activation, feed_dict={x: image_input})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(8,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")\n",
    "#with tf.Session() as sess:\n",
    "    #saver.restore(sess, save_file)\n",
    "    #conv2_map = sess.run(conv2_W, feed_dict={x: X_test_gray_own})\n",
    "    #print(sess.run(conv2_W))\n",
    "    #outputFeatureMap([X_test_gray_own[6]], conv1, plt_num=1)\n",
    "    #outputFeatureMap([X_test_gray_own[6]], conv2, plt_num=2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
